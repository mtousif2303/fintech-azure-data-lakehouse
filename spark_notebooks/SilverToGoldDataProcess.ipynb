{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-08-30T15:50:18.1353298Z",
              "execution_start_time": "2024-08-30T15:50:17.6016128Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "01dcd36c-9e8e-4c65-9391-5298725d2abf",
              "queued_time": "2024-08-30T15:50:17.4516709Z",
              "session_id": "6",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "gdssparkpool",
              "state": "finished",
              "statement_id": 8,
              "statement_ids": [
                8
              ]
            },
            "text/plain": [
              "StatementMeta(gdssparkpool, 6, 8, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def log_message(message):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "silver_base_path = \"abfss://fintech@fintechdatagdsde.dfs.core.windows.net/silver/\"\n",
        "\n",
        "log_message(\"Checking Silver layer structure...\")\n",
        "\n",
        "# List silver folder\n",
        "try:\n",
        "    folders = mssparkutils.fs.ls(silver_base_path)\n",
        "    log_message(f\"Folders in silver: {[f.name for f in folders]}\")\n",
        "except Exception as e:\n",
        "    log_message(f\"Error: {e}\")\n",
        "\n",
        "# Check Accounts folder contents\n",
        "try:\n",
        "    log_message(\"\\nChecking Accounts folder...\")\n",
        "    contents = mssparkutils.fs.ls(f\"{silver_base_path}Accounts/\")\n",
        "    log_message(f\"Items in Accounts: {len(contents)}\")\n",
        "    for item in contents[:10]:\n",
        "        log_message(f\"  {item.name} - {item.size} bytes\")\n",
        "    \n",
        "    has_delta = any('_delta_log' in item.name for item in contents)\n",
        "    log_message(f\"\\nHas _delta_log: {has_delta}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    log_message(f\"Error listing Accounts: {e}\")\n",
        "\n",
        "# Try reading as Parquet (not Delta)\n",
        "try:\n",
        "    log_message(\"\\nTrying to read as Parquet...\")\n",
        "    df = spark.read.parquet(f\"{silver_base_path}Accounts/\")\n",
        "    log_message(f\"SUCCESS! Read as Parquet - {df.count()} rows\")\n",
        "    df.printSchema()\n",
        "except Exception as e:\n",
        "    log_message(f\"Failed to read as Parquet: {e}\")\n",
        "\n",
        "# Try reading as Delta\n",
        "try:\n",
        "    log_message(\"\\nTrying to read as Delta...\")\n",
        "    df = spark.read.format(\"delta\").load(f\"{silver_base_path}Accounts/\")\n",
        "    log_message(f\"SUCCESS! Read as Delta - {df.count()} rows\")\n",
        "except Exception as e:\n",
        "    log_message(f\"Failed to read as Delta: {e}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
